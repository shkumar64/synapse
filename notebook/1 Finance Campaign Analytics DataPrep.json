{
	"name": "1 Finance Campaign Analytics DataPrep",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "fsi",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2
		},
		"metadata": {
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/4f560746-383b-468f-bdfa-bc5acc08a8f1/resourceGroups/DDiB-FSI-Lab/providers/Microsoft.Synapse/workspaces/synapsefsibnkhackbrku4oli5l4c2/bigDataPools/fsi",
				"name": "fsi",
				"type": "Spark",
				"endpoint": "https://synapsefsibnkhackbrku4oli5l4c2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/fsi",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"## *****For Demonstration purpose only, Please customize as per your enterprise security needs and compliances***** \n",
					"Disclaimer: By accessing this code, you acknowledge the code is made available for presentation and demonstration purposes only and that the code: (1) is not subject to SOC 1 and SOC 2 compliance audits; (2) is not designed or intended to be a substitute for the professional advice, diagnosis, treatment, or judgment of a certified financial services professional; (3) is not designed, intended or made available as a medical device; and (4) is not designed or intended to be a substitute for professional medical advice, diagnosis, treatment or judgement. Do not use this code to replace, substitute, or provide professional financial advice or judgment, or to replace, substitute or provide medical advice, diagnosis, treatment or judgement. You are solely responsible for ensuring the regulatory, legal, and/or contractual compliance of any use of the code, including obtaining any authorizations or consents, and any solution you choose to build that incorporates this code in whole or in part. \n"
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Please don't run / don't click \"Run all\" the notebook:\n",
					"At the time of writing of this document, the current core limit is 200 cores per workspace and depending upon number of concurrent users, you may end up with core capacity being exceeded or maximum number of parallel jobs being exceeded error. \n",
					"## Fetch Marketing Campaigns data into DataFrame and Calculate Revenue Variance        "
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"data_path = spark.read.load('abfss://marketing-data@stfsibnkhackbrku4oli5l4c.dfs.core.windows.net/CampaignData.csv', format='csv',header=True)\n",
					"data_path.show(10)"
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load into Pandas and Perform Cleansing Operations\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"\n",
					"import numpy as np\n",
					"\n",
					"pd_df = data_path.select(\"*\").toPandas()\n",
					"\n",
					"'''Cleansing Operations: \n",
					"1. Columns Revenue, Revenue_Target: Remove '$' symbol and convert datatype to float\n",
					"2. Columns Revenue, Revenue_Target: Replace null values with 0\n",
					"3. Columns Region, Country, Product_Category, Campaign_Name: Convert columns to Camel Case\n",
					"'''\n",
					"pd_df['Revenue']= pd_df['Revenue'].replace('[\\$,]', '', regex=True).astype(float)\n",
					"pd_df['Revenue_Target']= pd_df['Revenue_Target'].replace('[\\$,]', '', regex=True).astype(float)\n",
					"pd_df['Revenue'].fillna(value=0, inplace=True)\n",
					"pd_df['Revenue_Target'].fillna(value=0, inplace=True)\n",
					"\n",
					"pd_df['Region'] = pd_df.Region.str.title()\n",
					"pd_df['Country'] = pd_df.Country.str.title()\n",
					"\n",
					"pd_df['Campaign_Name'] = pd_df.Campaign_Name.str.title()"
				],
				"attachments": null,
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Data Transformation - Calculate Revenue Variance"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"#Create new column\n",
					"pd_df['Revenue_Variance'] = pd_df['Revenue_Target'] - pd_df['Revenue']\n",
					"\n",
					"print(pd_df[1:5])"
				],
				"attachments": null,
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Move data to Azure Data Lake Gen2\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"df = spark.createDataFrame(pd_df)\n",
					"df.show(5)\n",
					"\n",
					"(df\n",
					" .coalesce(1)\n",
					" .write\n",
					" .mode(\"overwrite\")\n",
					" .option(\"header\", \"true\")\n",
					" .format(\"com.databricks.spark.csv\")\n",
					" .save('abfss://marketing-data@stfsibnkhackbrku4oli5l4c.dfs.core.windows.net/Campaignsdata'))"
				],
				"attachments": null,
				"execution_count": 10
			}
		]
	}
}